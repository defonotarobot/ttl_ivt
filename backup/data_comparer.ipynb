{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import xlsxwriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26fd1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Config File\n",
    "with open(\"config_comparer.json\") as json_config_file:\n",
    "    config = json.load(json_config_file)\n",
    "\n",
    "# Initialize Variables\n",
    "config_source = config[\"source\"]\n",
    "config_target = config[\"target\"]\n",
    "default_unique_column = config[\"default_unique_column\"]\n",
    "config_faulty = config[\"faulty_dir\"]\n",
    "config_clean = config[\"clean_dir\"]\n",
    "config_output = config[\"output\"]\n",
    "exception_tables = config[\"exception_tables\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1b907684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(merged_table, column_number):\n",
    "\n",
    "    dfc = merged_table.drop(default_unique_column, axis=1)\n",
    "    result = merged_table[[default_unique_column]]\n",
    "    col = 2  # start from third col\n",
    "    for i in range(0, column_number-1):\n",
    "        # first colon selects all rows, second arg selects col-2 to col\n",
    "        com_col = dfc.iloc[:, col-2:col]\n",
    "        com_col['COM_' + com_col.columns[0][:-2]] = com_col.iloc[:, 0] == com_col.iloc[:, 1]\n",
    "\n",
    "        # comparison logic\n",
    "        # com_col column name defined on the left inside outer bracket\n",
    "        # [0] is the label and [:-2] says to get string from left until\n",
    "        # position -2 as counting from right increases negatively\n",
    "        # com_col values on the first column then set to bool comp logic of\n",
    "        col += 2\n",
    "        result = pd.concat([result, com_col], axis=1)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd21ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix(tbl, env1, env2):\n",
    "    result = tbl.copy()\n",
    "    for col in result.columns:\n",
    "        new_col = ''\n",
    "        if '_x' in col:\n",
    "            new_col = env1 + '_' + col.removesuffix('_x')\n",
    "        elif '_y' in col:\n",
    "            new_col = env2 + '_' + col.removesuffix('_y')\n",
    "        else:\n",
    "            new_col = col\n",
    "        result = result.rename(columns={col: new_col})\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cd865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(tbl, env1, env2):\n",
    "\n",
    "    y = tbl.copy()\n",
    "    y.insert(1, 'is in ' + env2, y[default_unique_column].isin(df2[default_unique_column]))\n",
    "    y.insert(1, 'is in ' + env1, y[default_unique_column].isin(df1[default_unique_column]))\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51f490d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clmpar.clpimvp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THEERA~1.LEE\\AppData\\Local\\Temp/ipykernel_26576/2570797495.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  com_col['COM_' + com_col.columns[0][:-2]] = com_col.iloc[:, 0] == com_col.iloc[:, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n",
      "clmpar.clpmodp.csv\n",
      "ID missing\n",
      "clmpar.clppar8.csv\n",
      "ID missing\n",
      "clmpar.clppari.csv\n",
      "ID missing\n",
      "clmpar.clpparj.csv\n",
      "ID missing\n",
      "clmpar.clppars.csv\n",
      "ID missing\n",
      "clmpar.clppgrt.csv\n",
      "ID missing\n",
      "clmpar.clppsta.csv\n",
      "ID missing\n",
      "clmpar.llpbotc.csv\n",
      "ID missing\n",
      "clmpar.llpfpty.csv\n",
      "ID missing\n",
      "clmpar.llppar9.csv\n",
      "ID missing\n",
      "clmpar.llpparb.csv\n",
      "ID missing\n",
      "clmpar.llppark.csv\n",
      "ID missing\n",
      "clmpar.llpparr.csv\n",
      "ID missing\n",
      "fispar.fippart.csv\n",
      "ID missing\n",
      "glspar.glpcoam.csv\n",
      "ID missing\n",
      "glspar.glpgref.csv\n",
      "ID missing\n",
      "glspar.glpint0.csv\n",
      "ID missing\n",
      "glspar.glpint1.csv\n",
      "ID missing\n",
      "glspar.glpint2.csv\n",
      "ID missing\n",
      "glspar.glpint3.csv\n",
      "ID missing\n",
      "glspar.glpint4.csv\n",
      "ID missing\n",
      "glspar.glpint5.csv\n",
      "ID missing\n",
      "glspar.glpint6.csv\n",
      "ID missing\n",
      "glspar.glppar3.csv\n",
      "ID missing\n",
      "glspar.glppar5.csv\n",
      "ID missing\n",
      "glspar.glppar7.csv\n",
      "ID missing\n",
      "glspar.glpparz.csv\n",
      "ID missing\n",
      "icspar.icppar2.csv\n",
      "ID missing\n",
      "icspar.icppar3.csv\n",
      "ID missing\n",
      "icspar.icppar4.csv\n",
      "ID missing\n",
      "icspar.icpparc.csv\n",
      "ID missing\n",
      "icspar.icpparg.csv\n",
      "ID missing\n",
      "icspar.icpparh.csv\n",
      "ID missing\n",
      "icspar.icpparj.csv\n",
      "ID missing\n",
      "icspar.icpparq.csv\n",
      "ID missing\n",
      "icspar.icpparr.csv\n",
      "ID missing\n",
      "icspar.icppars.csv\n",
      "ID missing\n",
      "icspar.icppart.csv\n",
      "ID missing\n",
      "icspar.icpparu.csv\n",
      "ID missing\n",
      "icspar.icpparv.csv\n",
      "ID missing\n",
      "icspar.icpparz.csv\n",
      "ID missing\n",
      "lexpar.lepblat.csv\n",
      "ID missing\n",
      "lexpar.lepblck.csv\n",
      "ID missing\n",
      "lnspar.lnppar0.csv\n",
      "ID missing\n",
      "lnspar.lnppar3.csv\n",
      "ID missing\n",
      "lnspar.lnppar6.csv\n",
      "ID missing\n",
      "lnspar.lnppard.csv\n",
      "ID missing\n",
      "lnspar.lnpparf.csv\n",
      "ID missing\n",
      "lnspar.lnppark.csv\n",
      "ID missing\n",
      "lnspar.lnpparp.csv\n",
      "ID missing\n",
      "lnspar.lnpprmr.csv\n",
      "ID missing\n",
      "lnspar.lnpprsp.csv\n",
      "ID missing\n",
      "syspar.sspasdsv.csv\n",
      "ID missing\n",
      "syspar.sspcldh.csv\n",
      "ID missing\n",
      "syspar.sspcoun.csv\n",
      "ID missing\n",
      "syspar.sspfxrp.csv\n",
      "ID missing\n",
      "syspar.sspfxrt.csv\n",
      "ID missing\n",
      "syspar.ssppar9.csv\n",
      "ID missing\n",
      "syspar.ssppare.csv\n",
      "ID missing\n",
      "syspar.ssprath.csv\n",
      "ID missing\n",
      "syspar.sspratx.csv\n",
      "ID missing\n",
      "syspar.sspstat.csv\n",
      "ID missing\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "directory = os.fsencode(config_source[\"dir\"])\n",
    "    \n",
    "for table in os.listdir(directory):\n",
    "    tablename = os.fsdecode(table)\n",
    "    print(tablename)\n",
    "    file = '\\\\'+tablename\n",
    "        \n",
    "    path1 = os.path.dirname(config_source[\"dir\"] + \"dummy.csv\")\n",
    "    path2 = os.path.dirname(config_target[\"dir\"] + \"dummy.csv\")\n",
    "    \n",
    "    env1 = os.path.basename(path1)\n",
    "    env2 = os.path.basename(path2)\n",
    "\n",
    "    df1 = pd.read_csv(path1+file,dtype=str).fillna('')\n",
    "    df2 = pd.read_csv(path2+file,dtype=str).fillna('')\n",
    "\n",
    "    id_present = default_unique_column in list(df1.columns)\n",
    "    if id_present == True:\n",
    "        dfx = df1.merge(df2, on = default_unique_column, how = 'outer') #outer join on id\n",
    "        tmp = dfx.reindex(sorted(dfx.columns), axis=1).fillna('') #sorting to make same field adjacent\n",
    "        x = compare(tmp,df1.shape[1]) #compare\n",
    "\n",
    "        almost_done = union(x,env1,env2)\n",
    "        done = prefix(almost_done,env1,env2)\n",
    "\n",
    "        with pd.ExcelWriter(config_output[\"dir\"] + env1 + '_' + env2 + '_' + tablename + '.xlsx') as writer:\n",
    "            df1.to_excel(writer, sheet_name=env1, index=False)\n",
    "            df2.to_excel(writer, sheet_name=env2, index=False)\n",
    "            done.to_excel(writer, sheet_name='COM', index=False)\n",
    "        print(\"Successful\")\n",
    "    else:\n",
    "        print(\"ID missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824a0a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
